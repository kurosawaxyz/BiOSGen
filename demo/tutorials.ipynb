{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7290f5fc",
   "metadata": {},
   "source": [
    "# Task 1: Data Preparation\n",
    "\n",
    "## Preliminary\n",
    "This task involves preparing the dataset for training the model. It includes data cleaning, normalization, and splitting the raw histology images into smaller ready-to-use patches. \n",
    "\n",
    "The reason why we need to split the images into patches is due to two main factors:\n",
    "1. **Memory Constraints**: Large images can be too big to fit into memory, especially when working with high-resolution histology images. A histology image contains a lot of information, with over 5000x5000 pixels. Processing them together, alongside with an exponentially larger network, currently there is no GPU that can handle this, the fact is that it's not even possible to process to the second epoch on a patch of size 1024x1024, which is nearly 5 times smaller than the original image.\n",
    "\n",
    "2. **Model Performance**: Smaller patches allow the model to focus on local features, which will indeniably improve the performance of the model. Supposing that there exists a GPU that can handle the full image, the model will still struggle to learn the local features, which are crucial for histology images. The model will be overwhelmed by the global features, which are not as important for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1aa72",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73d2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuzzsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
